{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d57b7d2",
   "metadata": {},
   "source": [
    "# Решение заданий (градиенты + простая сеть)\n",
    "\n",
    "В ноутбуке реализованы: Softmax, ReLU, Sigmoid, SimpleConv, CrossEntropy и NeuralNetwork.\n",
    "Также добавлены тесты grad_x через численную проверку градиента."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175347a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def _as_2d(x):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    if x.ndim == 1:\n",
    "        x = x[None, :]\n",
    "    return x\n",
    "\n",
    "class Layer:\n",
    "    def forward(self, x):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def grad_x(self, x, grad_out):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def grad_params(self, x, grad_out):\n",
    "        return []\n",
    "\n",
    "    def num_params(self):\n",
    "        return 0\n",
    "\n",
    "\n",
    "class LinearLayer(Layer):                       #Forward:y=xW+b\n",
    "                                                #Grad по входу: ∂L​/∂x=(∂L/∂y)*​W^T\n",
    "                                                #Grad по параметрам: ∂L/∂W​=x^T*(∂L​/∂y), ∂L/∂b​=∑_n (​∂L/∂Y_n​​)\n",
    "    def __init__(self, in_features, out_features, W=None, b=None, rng=None):\n",
    "        rng = np.random.default_rng() if rng is None else rng\n",
    "        if W is None:\n",
    "            \n",
    "            limit = np.sqrt(6.0 / (in_features + out_features))\n",
    "            W = rng.uniform(-limit, limit, size=(in_features, out_features))\n",
    "        if b is None:\n",
    "            b = np.zeros((out_features,), dtype=float)\n",
    "\n",
    "        self.W = np.asarray(W, dtype=float)\n",
    "        self.b = np.asarray(b, dtype=float)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x2 = _as_2d(x)\n",
    "        return x2 @ self.W + self.b\n",
    "\n",
    "    def grad_x(self, x, grad_out):\n",
    "        g = _as_2d(grad_out)\n",
    "        return g @ self.W.T\n",
    "\n",
    "    def grad_params(self, x, grad_out):\n",
    "        x2 = _as_2d(x)\n",
    "        g = _as_2d(grad_out)\n",
    "        dW = x2.T @ g\n",
    "        db = g.sum(axis=0)\n",
    "        return [(self.W, dW), (self.b, db)]\n",
    "\n",
    "    def num_params(self):\n",
    "        return self.W.size + self.b.size\n",
    "\n",
    "\n",
    "class ReLU(Layer):                          #Forward: ReLU(x)=max(0,x)\n",
    "                                            #Backward: ∂L/∂x​=(∂L/∂y)​⊙1(x>0)\n",
    "    def forward(self, x):\n",
    "        x2 = _as_2d(x)\n",
    "        return np.maximum(0.0, x2)\n",
    "\n",
    "    def grad_x(self, x, grad_out):\n",
    "        x2 = _as_2d(x)\n",
    "        g = _as_2d(grad_out)\n",
    "        return g * (x2 > 0.0)\n",
    "\n",
    "\n",
    "class Sigmoid(Layer):                       #Forward: σ(x)=1/1+e^(−x)\n",
    "                                            #Производная: σ′(x)=σ(x)(1−σ(x))\n",
    "    def forward(self, x):\n",
    "        x2 = _as_2d(x)\n",
    "        out = np.empty_like(x2)\n",
    "        pos = x2 >= 0\n",
    "        out[pos] = 1.0 / (1.0 + np.exp(-x2[pos]))\n",
    "        expx = np.exp(x2[~pos])\n",
    "        out[~pos] = expx / (1.0 + expx)\n",
    "        return out\n",
    "\n",
    "    def grad_x(self, x, grad_out):\n",
    "        y = self.forward(x)\n",
    "        g = _as_2d(grad_out)\n",
    "        return g * y * (1.0 - y)\n",
    "\n",
    "\n",
    "class Softmax(Layer):                       #Forward: y_i ​= e^(x_i)​​/∑_j(​e^(x_j)​)\n",
    "                                            #Backward: ∂y_i/∂x_j​​​=y_i​(δ_ij​ − y_i)\n",
    "    def forward(self, x):\n",
    "        x2 = _as_2d(x)\n",
    "        z = x2 - x2.max(axis=1, keepdims=True)\n",
    "        expz = np.exp(z)\n",
    "        return expz / expz.sum(axis=1, keepdims=True)\n",
    "\n",
    "    def grad_x(self, x, grad_out):\n",
    "        y = self.forward(x)\n",
    "        g = _as_2d(grad_out)\n",
    "        \n",
    "        dot = np.sum(g * y, axis=1, keepdims=True)\n",
    "        return y * (g - dot)\n",
    "\n",
    "\n",
    "class CrossEntropy:                          #Если у нас вероятности y_pred и one-hot t: L = −∑_i(​t_i*​log(y_i​))\n",
    "                                             #Градиент по y_pred: ∂L/∂y_i​=−(t_i​​/y_i​)\n",
    "    def __init__(self, eps=1e-8, reduction=\"mean\"):\n",
    "        self.eps = float(eps)\n",
    "        assert reduction in (\"mean\", \"sum\")\n",
    "        self.reduction = reduction\n",
    "\n",
    "    def __call__(self, y_pred, y_true):\n",
    "        return self.forward(y_pred, y_true)\n",
    "\n",
    "    def forward(self, y_pred, y_true):\n",
    "        y = _as_2d(y_pred)\n",
    "        t = _as_2d(y_true)\n",
    "        losses = -np.sum(t * np.log(y + self.eps), axis=1)\n",
    "        return float(losses.mean() if self.reduction == \"mean\" else losses.sum())\n",
    "\n",
    "    def grad_x(self, y_pred, y_true):\n",
    "        y = _as_2d(y_pred)\n",
    "        t = _as_2d(y_true)\n",
    "        g = -(t / (y + self.eps))\n",
    "        if self.reduction == \"mean\":\n",
    "            g = g / y.shape[0]\n",
    "        return g\n",
    "\n",
    "\n",
    "def one_hot(y, num_classes):\n",
    "    y = np.asarray(y, dtype=int).reshape(-1)\n",
    "    out = np.zeros((y.size, num_classes), dtype=float)\n",
    "    out[np.arange(y.size), y] = 1.0\n",
    "    return out\n",
    "\n",
    "\n",
    "class NeuralNetwork:                            #Forward + cache: сохраняем входы каждого слоя\n",
    "                                                #grad = dLoss/dOut\n",
    "    def __init__(self, layers, loss: CrossEntropy):\n",
    "        self.layers = list(layers)\n",
    "        self.loss = loss\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            out = layer.forward(out)\n",
    "        return out\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return self.forward(x)\n",
    "\n",
    "    def num_params(self):\n",
    "        return sum(getattr(l, \"num_params\", lambda: 0)() for l in self.layers)\n",
    "\n",
    "    def grad_x(self, x, y_true):\n",
    "        \n",
    "        xs = []\n",
    "        out = x\n",
    "        for layer in self.layers:\n",
    "            xs.append(out)\n",
    "            out = layer.forward(out)\n",
    "\n",
    "        grad = self.loss.grad_x(out, y_true)\n",
    "        for layer, x_in in zip(reversed(self.layers), reversed(xs)):\n",
    "            grad = layer.grad_x(x_in, grad)\n",
    "        return grad\n",
    "\n",
    "    def train_one_step(self, X_batch, Y_batch, learning_rate):\n",
    "        \n",
    "        xs = []\n",
    "        out = X_batch\n",
    "        for layer in self.layers:\n",
    "            xs.append(out)\n",
    "            out = layer.forward(out)\n",
    "\n",
    "        grad = self.loss.grad_x(out, Y_batch)\n",
    "\n",
    "        # backward + SGD\n",
    "        for layer, x_in in zip(reversed(self.layers), reversed(xs)):\n",
    "            for param, dparam in layer.grad_params(x_in, grad):\n",
    "                param -= learning_rate * dparam\n",
    "            grad = layer.grad_x(x_in, grad)\n",
    "\n",
    "    def fit_one_epoch(self, X, Y, batch_size, learning_rate, shuffle=True, rng=None):\n",
    "        rng = np.random.default_rng() if rng is None else rng\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        Y = np.asarray(Y, dtype=float)\n",
    "\n",
    "        n = X.shape[0]\n",
    "        idx = np.arange(n)\n",
    "        if shuffle:\n",
    "            rng.shuffle(idx)\n",
    "\n",
    "        for start in range(0, n, batch_size):\n",
    "            bi = idx[start:start + batch_size]\n",
    "            self.train_one_step(X[bi], Y[bi], learning_rate)\n",
    "\n",
    "\n",
    "class SimpleConv(Layer):                                #Forward: y[i,j]=∑_(u,v)(​x[i+u,j+v]⋅K[u,v]+b)\n",
    "                                                        #dL/dx: dx[i+u,j+v]+=go[i,j]⋅K[u,v]\n",
    "                                                        #dL/dK: dK[u,v]+=go[i,j]⋅x[i+u,j+v]\n",
    "    \n",
    "    def __init__(self, kernel, bias=0.0):\n",
    "        self.K = np.asarray(kernel, dtype=float)        \n",
    "        self.b = np.asarray([bias], dtype=float)        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        if x.ndim == 2:\n",
    "            x = x[None, :, :]\n",
    "\n",
    "        N, H, W = x.shape\n",
    "        kh, kw = self.K.shape\n",
    "        out_h, out_w = H - kh + 1, W - kw + 1\n",
    "\n",
    "        y = np.zeros((N, out_h, out_w), dtype=float)\n",
    "        for n in range(N):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    patch = x[n, i:i+kh, j:j+kw]\n",
    "                    y[n, i, j] = np.sum(patch * self.K) + self.b[0]\n",
    "        return y\n",
    "\n",
    "    def grad_x(self, x, grad_out):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        if x.ndim == 2:\n",
    "            x = x[None, :, :]\n",
    "        go = np.asarray(grad_out, dtype=float)\n",
    "        if go.ndim == 2:\n",
    "            go = go[None, :, :]\n",
    "\n",
    "        N, H, W = x.shape\n",
    "        kh, kw = self.K.shape\n",
    "        out_h, out_w = H - kh + 1, W - kw + 1\n",
    "\n",
    "        dx = np.zeros_like(x)\n",
    "        for n in range(N):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    dx[n, i:i+kh, j:j+kw] += go[n, i, j] * self.K\n",
    "        return dx\n",
    "\n",
    "    def grad_params(self, x, grad_out):\n",
    "        x = np.asarray(x, dtype=float)\n",
    "        if x.ndim == 2:\n",
    "            x = x[None, :, :]\n",
    "        go = np.asarray(grad_out, dtype=float)\n",
    "        if go.ndim == 2:\n",
    "            go = go[None, :, :]\n",
    "\n",
    "        N, H, W = x.shape\n",
    "        kh, kw = self.K.shape\n",
    "        out_h, out_w = H - kh + 1, W - kw + 1\n",
    "\n",
    "        dK = np.zeros_like(self.K)\n",
    "        db = 0.0\n",
    "        for n in range(N):\n",
    "            for i in range(out_h):\n",
    "                for j in range(out_w):\n",
    "                    patch = x[n, i:i+kh, j:j+kw]\n",
    "                    dK += go[n, i, j] * patch\n",
    "                    db += go[n, i, j]\n",
    "\n",
    "        return [(self.K, dK), (self.b, np.asarray([db], dtype=float))]\n",
    "\n",
    "    def num_params(self):\n",
    "        return self.K.size + self.b.size\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6952305",
   "metadata": {},
   "source": [
    "## Тесты grad_x (finite differences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a3abbd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Все grad_x тесты прошли\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def finite_diff_grad(f, x, eps=1e-6):\n",
    "    x = np.asarray(x, dtype=float)\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old = x[idx]\n",
    "        x[idx] = old + eps\n",
    "        f1 = f(x)\n",
    "        x[idx] = old - eps\n",
    "        f2 = f(x)\n",
    "        x[idx] = old\n",
    "        grad[idx] = (f1 - f2) / (2 * eps)\n",
    "        it.iternext()\n",
    "    return grad\n",
    "\n",
    "def assert_close(a, b, tol=1e-5, name=''):\n",
    "    a = np.asarray(a); b = np.asarray(b)\n",
    "    err = np.max(np.abs(a - b))\n",
    "    if err > tol:\n",
    "        raise AssertionError(f'{name} max_err={err} > {tol}')\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# 1) LinearLayer.grad_x\n",
    "x = rng.normal(size=(3, 5))\n",
    "up = rng.normal(size=(3, 4))\n",
    "lin = LinearLayer(5, 4, rng=rng)\n",
    "g_num = finite_diff_grad(lambda xx: float(np.sum(lin.forward(xx) * up)), x.copy())\n",
    "g_an = lin.grad_x(x, up)\n",
    "assert_close(g_num, g_an, 1e-5, 'LinearLayer.grad_x')\n",
    "\n",
    "# 2) ReLU.grad_x\n",
    "relu = ReLU()\n",
    "x = rng.normal(size=(2, 6))\n",
    "up = rng.normal(size=(2, 6))\n",
    "g_num = finite_diff_grad(lambda xx: float(np.sum(relu.forward(xx) * up)), x.copy())\n",
    "g_an = relu.grad_x(x, up)\n",
    "assert_close(g_num, g_an, 1e-5, 'ReLU.grad_x')\n",
    "\n",
    "# 3) Softmax.grad_x\n",
    "sm = Softmax()\n",
    "x = rng.normal(size=(2, 7))\n",
    "up = rng.normal(size=(2, 7))  # dS/dy\n",
    "g_num = finite_diff_grad(lambda xx: float(np.sum(sm.forward(xx) * up)), x.copy())\n",
    "g_an = sm.grad_x(x, up)\n",
    "assert_close(g_num, g_an, 1e-5, 'Softmax.grad_x')\n",
    "\n",
    "# 4) CrossEntropy.grad_x (по y_pred)\n",
    "ce = CrossEntropy(eps=1e-8, reduction='mean')\n",
    "y = sm.forward(rng.normal(size=(4, 5)))\n",
    "t = one_hot(rng.integers(0, 5, size=4), 5)\n",
    "g_num = finite_diff_grad(lambda yy: ce.forward(yy.reshape(4, 5), t), y.copy()).reshape(4, 5)\n",
    "g_an = ce.grad_x(y, t)\n",
    "assert_close(g_num, g_an, 1e-5, 'CrossEntropy.grad_x')\n",
    "\n",
    "# 5) NeuralNetwork.grad_x\n",
    "layers = [LinearLayer(5, 8, rng=rng), ReLU(), LinearLayer(8, 3, rng=rng), Softmax()]\n",
    "net = NeuralNetwork(layers, loss=CrossEntropy(eps=1e-8))\n",
    "\n",
    "x = rng.normal(size=(1, 5))\n",
    "t = one_hot(np.array([1]), 3)\n",
    "\n",
    "g_num = finite_diff_grad(lambda xx: net.loss.forward(net.forward(xx.reshape(1, 5)), t), x.copy()).reshape(1, 5)\n",
    "g_an = net.grad_x(x, t)\n",
    "assert_close(g_num, g_an, 1e-5, 'NeuralNetwork.grad_x')\n",
    "\n",
    "print('✅ Все grad_x тесты прошли')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c966aba4",
   "metadata": {},
   "source": [
    "## Мини-тест на MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "885759b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 acc=0.9225\n",
      "epoch=2 acc=0.9367\n",
      "epoch=3 acc=0.9464\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def load_data(path='mnist.npz'):\n",
    "    with np.load(path, allow_pickle=True) as f:\n",
    "        x_train, y_train = f['x_train'], f['y_train']\n",
    "        x_test, y_test = f['x_test'], f['y_test']\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = load_data('mnist.npz')\n",
    "\n",
    "x_train = x_train.reshape((x_train.shape[0], -1)).astype(float) / 255.0\n",
    "x_test = x_test.reshape((x_test.shape[0], -1)).astype(float) / 255.0\n",
    "\n",
    "y_train_oh = one_hot(y_train, 10)\n",
    "y_test_oh = one_hot(y_test, 10)\n",
    "\n",
    "def accuracy(net, x, y_labels):\n",
    "    probs = net.forward(x)\n",
    "    pred = np.argmax(probs, axis=1)\n",
    "    return float(np.mean(pred == y_labels))\n",
    "\n",
    "layers = [\n",
    "    LinearLayer(784, 100),\n",
    "    ReLU(),\n",
    "    LinearLayer(100, 10),\n",
    "    Softmax(),\n",
    "]\n",
    "net = NeuralNetwork(layers, loss=CrossEntropy(eps=1e-8))\n",
    "\n",
    "for epoch in range(3):\n",
    "    net.fit_one_epoch(X=x_train, Y=y_train_oh, batch_size=64, learning_rate=0.05)\n",
    "    print(f'epoch={epoch+1} acc={accuracy(net, x_test, y_test):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feee091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before: test_acc=0.1779 test_loss=2.3203\n",
      "epoch=01 train_acc=0.9148 train_loss=0.2960 | test_acc=0.9177 test_loss=0.2882\n",
      "epoch=02 train_acc=0.9335 train_loss=0.2325 | test_acc=0.9349 test_loss=0.2282\n",
      "epoch=03 train_acc=0.9438 train_loss=0.1991 | test_acc=0.9431 test_loss=0.2006\n",
      "epoch=04 train_acc=0.9521 train_loss=0.1698 | test_acc=0.9507 test_loss=0.1719\n",
      "epoch=05 train_acc=0.9567 train_loss=0.1517 | test_acc=0.9541 test_loss=0.1563\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def load_mnist_npz(path=\"mnist.npz\"):\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"Файл '{path}' не найден. Положи mnist.npz рядом с ноутбуком или укажи полный путь.\"\n",
    "        )\n",
    "    with np.load(path, allow_pickle=True) as f:\n",
    "        x_train, y_train = f[\"x_train\"], f[\"y_train\"]\n",
    "        x_test, y_test = f[\"x_test\"], f[\"y_test\"]\n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def preprocess_mnist(x):\n",
    "    \n",
    "    x = x.reshape((x.shape[0], -1)).astype(np.float32) / 255.0\n",
    "    return x\n",
    "\n",
    "\n",
    "def evaluate(net, loss_fn, X, y_labels, y_onehot=None, batch_size=2048):\n",
    "    n = X.shape[0]\n",
    "    correct = 0\n",
    "    total_loss = 0.0\n",
    "\n",
    "    for start in range(0, n, batch_size):\n",
    "        xb = X[start:start+batch_size]\n",
    "        yb = y_labels[start:start+batch_size]\n",
    "\n",
    "        probs = net.forward(xb)                 \n",
    "        pred = np.argmax(probs, axis=1)\n",
    "        correct += int(np.sum(pred == yb))\n",
    "\n",
    "        if y_onehot is not None:\n",
    "            yoh = y_onehot[start:start+batch_size]\n",
    "            \n",
    "            total_loss += float(loss_fn.forward(probs, yoh)) * xb.shape[0]\n",
    "\n",
    "    acc = correct / n\n",
    "    avg_loss = (total_loss / n) if y_onehot is not None else None\n",
    "    return acc, avg_loss\n",
    "\n",
    "\n",
    "def build_mnist_net(hidden=128, seed=42):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    layers = [\n",
    "        LinearLayer(784, hidden, rng=rng),\n",
    "        ReLU(),\n",
    "        LinearLayer(hidden, 10, rng=rng),\n",
    "        Softmax(),\n",
    "    ]\n",
    "    loss_fn = CrossEntropy(eps=1e-8, reduction=\"mean\")\n",
    "    net = NeuralNetwork(layers, loss=loss_fn)\n",
    "    return net, loss_fn\n",
    "\n",
    "\n",
    "(x_train_raw, y_train), (x_test_raw, y_test) = load_mnist_npz(\"mnist.npz\")\n",
    "x_train = preprocess_mnist(x_train_raw)\n",
    "x_test = preprocess_mnist(x_test_raw)\n",
    "\n",
    "y_train_oh = one_hot(y_train, 10)\n",
    "y_test_oh = one_hot(y_test, 10)\n",
    "\n",
    "net, loss_fn = build_mnist_net(hidden=128, seed=42)\n",
    "\n",
    "\n",
    "test_acc, test_loss = evaluate(net, loss_fn, x_test, y_test, y_onehot=y_test_oh)\n",
    "print(f\"before: test_acc={test_acc:.4f} test_loss={test_loss:.4f}\")\n",
    "\n",
    "\n",
    "EPOCHS = 5\n",
    "LR = 0.05\n",
    "BATCH = 64\n",
    "rng = np.random.default_rng(123) \n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    net.fit_one_epoch(X=x_train, Y=y_train_oh, batch_size=BATCH, learning_rate=LR, shuffle=True, rng=rng)\n",
    "\n",
    "    train_acc, train_loss = evaluate(net, loss_fn, x_train, y_train, y_onehot=y_train_oh)\n",
    "    test_acc, test_loss = evaluate(net, loss_fn, x_test, y_test, y_onehot=y_test_oh)\n",
    "\n",
    "    print(f\"epoch={epoch:02d} \"\n",
    "          f\"train_acc={train_acc:.4f} train_loss={train_loss:.4f} | \"\n",
    "          f\"test_acc={test_acc:.4f} test_loss={test_loss:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
